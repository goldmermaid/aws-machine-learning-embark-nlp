{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs) for the Regression Problem\n",
    "\n",
    "In this notebook, we build a Recurrent Neural Networks (RNNs) using GloVe word embeddings, to predict the __log_votes__ field of our review dataset.\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __rating:__ Rating of the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "\n",
    "__Important note:__ One big distinction betweeen the regular neural networks and RNNs is that RNNs work with sequential data. In our case, RNNs will help us with the text field. If we also want to consider other fields such as time, rating, verified, etc. , we need to use the regular neural networks and connect it to the RNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import d2l\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd, np, npx\n",
    "from mxnet.gluon import nn, rnn\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset below and fill-in the reviewText field. We will use this field as input to our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../DATA/NLP/EMBK-NLP-REVIEW-DATA-CSV.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five rows in the dataset. As you can see the __log_votes__ field is numeric. That's why we will build a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>rating</th>\n",
       "      <th>log_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuck with this at work, slow and we still got...</td>\n",
       "      <td>Use SEP or Mcafee</td>\n",
       "      <td>False</td>\n",
       "      <td>1464739200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I use parallels every day with both my persona...</td>\n",
       "      <td>Use it daily</td>\n",
       "      <td>False</td>\n",
       "      <td>1332892800</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barbara Robbins\\n\\nI've used TurboTax to do ou...</td>\n",
       "      <td>Helpful Product</td>\n",
       "      <td>True</td>\n",
       "      <td>1398816000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have been using this software security for y...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1430784000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>If you want your computer hijacked and slowed ...</td>\n",
       "      <td>... hijacked and slowed to a crawl Windows 10 ...</td>\n",
       "      <td>False</td>\n",
       "      <td>1508025600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  Stuck with this at work, slow and we still got...   \n",
       "1  I use parallels every day with both my persona...   \n",
       "2  Barbara Robbins\\n\\nI've used TurboTax to do ou...   \n",
       "3  I have been using this software security for y...   \n",
       "4  If you want your computer hijacked and slowed ...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                                  Use SEP or Mcafee     False  1464739200   \n",
       "1                                       Use it daily     False  1332892800   \n",
       "2                                    Helpful Product      True  1398816000   \n",
       "3                                         Five Stars      True  1430784000   \n",
       "4  ... hijacked and slowed to a crawl Windows 10 ...     False  1508025600   \n",
       "\n",
       "   rating  log_votes  \n",
       "0     1.0        0.0  \n",
       "1     5.0        0.0  \n",
       "2     4.0        0.0  \n",
       "3     5.0        0.0  \n",
       "4     1.0        0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis and Missing Value Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the range and distribution of log_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"log_votes\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.799753318287247"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum = df[\"log_votes\"].max()\n",
    "maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df[\"log_votes\"].plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviewText    6\n",
      "summary       7\n",
      "verified      0\n",
      "time          0\n",
      "rating        0\n",
      "log_votes     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only consider the reviewText field. Let's fill-in the missing values for that below. We will just use the placeholder \"Missing\" here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviewText\"].fillna(\"Missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## length of dataset: 55000\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text processing-cleaning\n",
    "Next, we will clean the text. We will remove leading/train white space, extra space and html tags. Recurrent neural networks usually __DON'T__ need text processing work further than simple text cleaning. Stemming and lemmatization can introduce some errors that will cause our model to skip those words completely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some string preprocessing\n",
    "def clean_str(text):\n",
    "    text = text.lower().strip() # Remove leading/trailing whitespace\n",
    "    text = re.sub('\\s+', ' ', text) # Remove extra space and tabs\n",
    "    text = re.compile('<.*?>').sub('', text) # Remove HTML tags/markups:\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to process all of the words in the reviews, count the number of occurences of each word, and then index the words in descending order with respect to how many times this occur. This is a necessary input to help us encode the words in the reviews so that they can be understood by a machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a dictionary of the words and their counts\n",
    "\n",
    "word_counter = Counter()\n",
    "def create_count(sentiments):\n",
    "    for line in sentiments:\n",
    "        for word in (clean_str(line)).split():\n",
    "            if word not in word_counter.keys():               \n",
    "                word_counter[word] = 1\n",
    "            else:\n",
    "                word_counter[word] += 1\n",
    "\n",
    "#This assigns a unique a number for each word (sorted by descending order \n",
    "#based on the frequency of occurrence)and returns a word_dict\n",
    "\n",
    "def create_word_index():\n",
    "    idx = 1\n",
    "    word_dict = {}\n",
    "    for word in word_counter.most_common():\n",
    "        word_dict[word[0]] = idx\n",
    "        idx+=1\n",
    "    return word_dict\n",
    "    \n",
    "#Here we combine all of the reviews into one dataset and create a word\n",
    "#dictionary using this entire dataset\n",
    "create_count(df[\"reviewText\"].tolist())\n",
    "word_dict = create_word_index()\n",
    "\n",
    "#This creates a reverse index from a number to the word \n",
    "idx2word = {v: k for k, v in word_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a set of helper functions that (1) encode words into a sequence of numbers, (2) decode a sequence of numbers back into words, and (3) truncate and pad the input data to ensure they are of equal length and thereby enable easier processing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This helper function creates a encoded sentences by assigning the unique \n",
    "#id from word_dict to the words in the input text\n",
    "def encoded_sentences(input_file,word_dict):\n",
    "    output_string = []\n",
    "    for line in input_file:\n",
    "        output_line = []\n",
    "        for word in (clean_str(line)).split():\n",
    "            if word in word_dict:\n",
    "                output_line.append(word_dict[word])\n",
    "        output_string.append(output_line)\n",
    "    return output_string\n",
    "\n",
    "#This helper function decodes encoded sentences\n",
    "def decode_sentences(input_file,word_dict):\n",
    "    output_string = []\n",
    "    for line in input_file:\n",
    "        output_line = ''\n",
    "        for idx in line:\n",
    "            output_line += idx2word[idx] + ' '\n",
    "        output_string.append(output_line)\n",
    "    return output_string\n",
    "\n",
    "#This helper function pads the sequences to maxlen.\n",
    "#If the sentence is greater than maxlen, it truncates the sentence.\n",
    "#If the sentence is less than 50, it pads with value 0.\n",
    "def pad_sequences(sentences,maxlen=50,value=0):\n",
    "    padded_sentences = []\n",
    "    for sen in sentences:\n",
    "        new_sentence = []\n",
    "        if(len(sen) > maxlen):\n",
    "            new_sentence = sen[:maxlen]\n",
    "            padded_sentences.append(new_sentence)\n",
    "        else:\n",
    "            num_padding = maxlen - len(sen)\n",
    "            new_sentence = np.append(sen,[value] * num_padding)\n",
    "            padded_sentences.append(new_sentence)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are going to encode all of the reviewText using the word dictionary created. In addition, we are going to cap the size of the tracked vocabulary size - meaning any word that is outside of the tracked range will be encoded with the last position. This is performance versus accuracy consideration - a larger tracked vocabulary will lead to more accurary but will have performance considerations because it requires a longer training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's encode sentences\n",
    "encoded_texts = encoded_sentences(df[\"reviewText\"].tolist(), word_dict)\n",
    "\n",
    "#Here we set the total num of words to be tracked\n",
    "vocab_size = 5000 \n",
    "\n",
    "#Any word outside of the tracked range will be encoded with last position.\n",
    "# t_data = [np.array([i if i<(vocab_size-1) else (vocab_size-1) for i in s]) for s in encoded_texts]\n",
    "all_labels = df[\"log_votes\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1095, 15, 10, 42, 751, 449, 4, 102, 101, 117, 1147, 15, 5, 10733, 12853]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.095e+03, 1.500e+01, 1.000e+01, 4.200e+01, 7.510e+02, 4.490e+02,\n",
       "       4.000e+00, 1.020e+02, 1.010e+02, 1.170e+02, 1.147e+03, 1.500e+01,\n",
       "       5.000e+00, 4.999e+03, 4.999e+03])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's print the first sentence\n",
    "# We have 4999 for words outside range of 5000 words\n",
    "\n",
    "# t_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Using pre-trained GloVe Word Embeddings:\n",
    "\n",
    "In this example, we will use GloVe word vectors. The following code shows how to get the word vectors and create an embedding dictionary with them. The dictionary maps the words to their word vectors. The file is downloaded from here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we first create a mapper for the word vectors (word->vector) with the __load_glove_index()__ function. Later, __create_emb()__ creates an embedding matrix. Each row corresponds to a word. For our vocabulary size of 5000 and 300 word vector dimension, this gives a matrix of 5000 rows and  300 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2L embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This separates 15% of the entire dataset into test dataset.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"reviewText\"], all_labels, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46750,) 8250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Stuck with this at work, slow and we still got hit with a ransomware attack.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train.shape, len(y_test))\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(X_train, X_test, y_train, y_test, num_steps=50):\n",
    "    ## num_steps=50 trim the sentence after the 50th word\n",
    "    \n",
    "    train_tokens = d2l.tokenize(X_train, token='word')\n",
    "    test_tokens = d2l.tokenize(X_test, token='word')\n",
    "    vocab = d2l.Vocab(train_tokens, min_freq=5)\n",
    "    \n",
    "    ## convert to ndarray\n",
    "    train_features = np.array([d2l.trim_pad(vocab[line], num_steps, vocab.unk)\n",
    "                               for line in train_tokens], dtype=np.float32)\n",
    "    test_features = np.array([d2l.trim_pad(vocab[line], num_steps, vocab.unk)\n",
    "                              for line in test_tokens], dtype=np.float32)  ## l2_loss does not accept float64\n",
    "    y_train = np.array(y_train, dtype=np.float32).reshape(-1, 1)\n",
    "    y_test = np.array(y_test, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "    return train_features, test_features, y_train, y_test, vocab\n",
    "\n",
    "truncate_word_after_max = 50\n",
    "train_features, test_features, train_labels, test_labels, vocab = \\\n",
    "    load_data(X_train, X_test, y_train, y_test, truncate_word_after_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.35235381126404\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "from mxnet.contrib import text\n",
    "glove_embedding = text.embedding.create('glove', pretrained_file_name='glove.6B.300d.txt')\n",
    "embedding_matrix = glove_embedding.get_vecs_by_tokens(vocab.idx_to_token)\n",
    "embedding_matrix.shape\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "context, num_hidden = mx.cpu(), 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Embedding(23421 -> 300, float32)\n",
       "  (1): RNN(-1 -> 100, NTC)\n",
       "  (2): Dense(-1 -> 1, Activation(relu))\n",
       ")"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "model.add(nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1]),        # Embedding layer\n",
    "          # Recurrent layer: where T, N and C stand for sequence length, \n",
    "          # batch size, and feature dimensions respectively.\n",
    "          rnn.RNN(num_hidden, layout = 'NTC'),        \n",
    "          nn.Dense(1, activation='relu')              # Output layer\n",
    "         )\n",
    "\n",
    "model.collect_params().initialize(mx.init.Xavier(), ctx=context)\n",
    "model[0].weight.set_data(embedding_matrix)\n",
    "model[0].collect_params().setattr('grad_req', 'null')\n",
    "model\n",
    "\n",
    "\n",
    "# class BiRNN(nn.Block):\n",
    "#     def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "#                  num_layers, **kwargs):\n",
    "#         super(BiRNN, self).__init__(**kwargs)\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "#         self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,\n",
    "#                                 bidirectional=True, input_size=embed_size)\n",
    "#         self.decoder = nn.Dense(1)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         # The shape of inputs is (batch size, number of words). Because LSTM\n",
    "#         # needs to use sequence as the first dimension, the input is\n",
    "#         # transformed and the word feature is then extracted. The output shape\n",
    "#         # is (number of words, batch size, word vector dimension).\n",
    "#         embeddings = self.embedding(inputs.T)\n",
    "#         # Since the input (embeddings) is the only argument passed into\n",
    "#         # rnn.LSTM, it only returns the hidden states of the last hidden layer\n",
    "#         # at different timestep (outputs). The shape of outputs is\n",
    "#         # (number of words, batch size, 2 * number of hidden units).\n",
    "#         outputs = self.encoder(embeddings)\n",
    "#         # Concatenate the hidden states of the initial timestep and final\n",
    "#         # timestep to use as the input of the fully connected layer. Its\n",
    "#         # shape is (batch size, 4 * number of hidden units)\n",
    "#         print(outputs.shape)\n",
    "#         encoding = np.concatenate((outputs[0], outputs[-1]), axis= -1)\n",
    "#         print(encoding.shape)\n",
    "#         outs = self.decoder(encoding)\n",
    "#         return outs\n",
    "    \n",
    "# model = BiRNN(embedding_matrix.shape[0], embedding_matrix.shape[1], 100, 2)    \n",
    "# # model = BiRNN(vocab_size=embedding_matrix.shape[0], \n",
    "# #             embed_size=embedding_matrix.shape[1], \n",
    "# #             num_hiddens=100, num_layers=2)\n",
    "# model.initialize(mx.init.Xavier(), ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_rmse(net, features, labels, maximum=maximum):\n",
    "    # To further stabilize the value when the logarithm is taken, set the\n",
    "    # value less than 1 as 1\n",
    "#     out = [net(X) for X in features]\n",
    "    clipped_preds = np.clip(out, 1, maximum)\n",
    "    return np.sqrt(2 * l2_loss(np.log(clipped_preds), np.log(labels)).mean())\n",
    "\n",
    "def train(net, train_features, train_labels, test_features, test_labels,\n",
    "          num_epochs, learning_rate, batch_size):\n",
    "    l2_loss = gluon.loss.L2Loss() \n",
    "    train_iter = d2l.load_array((train_features, train_labels), batch_size)\n",
    "    test_iter = d2l.load_array((test_features, test_labels), batch_size)\n",
    "    # The SGD optimization algorithm is used here\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', \n",
    "                            {'learning_rate': learning_rate})\n",
    "    for epoch in range(num_epochs):\n",
    "        train_ls, test_ls = 0, 0\n",
    "        for X, y in train_iter:\n",
    "#             print(X.shape) # (64, 50)\n",
    "            with autograd.record():\n",
    "                out = net(X)\n",
    "                l = l2_loss(out, y)\n",
    "                \n",
    "                l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            train_ls += l.sum()\n",
    "#         train_ls.append(log_rmse(net, train_features, train_labels))\n",
    "        \n",
    "        \n",
    "        for test_X, test_y in test_iter:\n",
    "            test_ls += l2_loss(net(test_X), test_y).sum()\n",
    "            \n",
    "        # Let's take the average losses\n",
    "        training_loss = train_ls / len(train_labels)\n",
    "        val_loss = test_ls / len(test_labels)\n",
    "        print(\"Epoch %s. Train_loss (mse) %s Validation_loss (mse) %s\" % (epoch, training_loss, val_loss))\n",
    "    return training_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss (mse) 0.3999576 Validation_loss (mse) 0.4121995\n",
      "Epoch 1. Train_loss (mse) 0.3727305 Validation_loss (mse) 0.40825984\n",
      "Epoch 2. Train_loss (mse) 0.3615286 Validation_loss (mse) 0.40524653\n",
      "Epoch 3. Train_loss (mse) 0.35311273 Validation_loss (mse) 0.41033566\n",
      "Epoch 4. Train_loss (mse) 0.34595266 Validation_loss (mse) 0.40895277\n",
      "Epoch 5. Train_loss (mse) 0.3397984 Validation_loss (mse) 0.40695733\n",
      "Epoch 6. Train_loss (mse) 0.33354 Validation_loss (mse) 0.40451565\n",
      "Epoch 7. Train_loss (mse) 0.32762694 Validation_loss (mse) 0.4118137\n",
      "Epoch 8. Train_loss (mse) 0.32276928 Validation_loss (mse) 0.43198064\n",
      "Epoch 9. Train_loss (mse) 0.3176806 Validation_loss (mse) 0.41089064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(14851.567), array(3389.8477))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate, epochs, batch_size = 0.01, 10, 64\n",
    "\n",
    "\n",
    "train(net=model, train_features=train_features, train_labels=train_labels, \n",
    "      test_features=test_features, test_labels=test_labels,\n",
    "      num_epochs=epochs, learning_rate=learning_rate, \n",
    "      batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features.shape)\n",
    "print(y_train.shape)\n",
    "# model(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    training_loss, val_loss = [], []\n",
    "#     train_predictions = np.array([])\n",
    "    # Training loop, train the network\n",
    "    for idx,(data,target) in enumerate(train_iter):\n",
    "\n",
    "        data = data.as_in_context(context)\n",
    "        target = target.as_in_context(context)\n",
    "\n",
    "        with autograd.record():\n",
    "            L = l2_loss(model(data), target)\n",
    "#             print(L.sum())\n",
    "#             training_loss += L.sum() #.asscalar()\n",
    "            #print(idx, training_loss)\n",
    "#             if len(train_predictions)>0:\n",
    "#                 train_predictions = np.concatenate((train_predictions, output), axis=0)\n",
    "#             else:\n",
    "#                 train_predictions = output\n",
    "\n",
    "        L.backward()\n",
    "        trainer.step(batch_size)\n",
    "    training_loss.append(log_rmse(model, train_features, y_train))\n",
    "\n",
    "#     for (test_data, test_target) in test_iter:\n",
    "    val_loss.append(log_rmse(model, test_features, y_test))\n",
    "    print(\"Epoch %s. Train_loss (mse) %s Validation_loss (mse) %s\" % (epoch, training_loss, val_loss))\n",
    "        \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target.sum().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(test_features[0].as_in_context(context))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_predictions = [model(X) for X in test_features]    \n",
    "val_loss = l2_loss(val_predictions, y_test).sum().asscalar()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    training_loss = 0\n",
    "    # Training loop, train the network\n",
    "    for idx,(data,target) in enumerate(train_iter):\n",
    "\n",
    "        data = data.as_in_context(context)\n",
    "        target = target.as_in_context(context)\n",
    "        \n",
    "        with autograd.record():\n",
    "            output = model(data)\n",
    "            \n",
    "            L = l2_loss(output, target)\n",
    "            training_loss += L.sum().asscalar()            \n",
    "            L.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "    \n",
    "    val_predictions = [model(X) for X in test_features]    \n",
    "    val_loss = l2_loss(val_predictions, y_test).sum().asscalar()\n",
    "    \n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(X_train)\n",
    "    val_loss = val_loss / len(val_predictions)\n",
    "      \n",
    "    print(\"Epoch %s. Train_loss (mse) %s Validation_loss (mse) %s\" % (epoch, training_loss, val_loss))\n",
    "    \n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we prepare the review texts to be fed into the deep learning model by (1) Reserving 15% of the dataset as a validation dataset, (2) padding and truncating the data to the length of 50 words, and (3) converting the encoded text into into MXNet's NDArray format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set our parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden = 64\n",
    "learning_rate = .001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using an RNN model with 64 hidden units. Let's use the Sequential mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we execute the training loop, we need to define a function that will calculate the accurary metrics for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the training process below. We will print Mean Squared Error after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mse(data_iterator, net, ctx=mx.cpu()):\n",
    "    metric = mx.metric.MSE()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data1 = batch.data[0].as_in_context(ctx)\n",
    "        data2 = batch.data[1].as_in_context(ctx)\n",
    "        data = [data1, data2]\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        metric.update([label], [output])\n",
    "    return metric.get()[1]\n",
    "\n",
    "def evaluate_accuracy(data_iterator, net, ctx=mx.cpu()):\n",
    "    metric = mx.metric.MSE()\n",
    "    data_iterator.reset()\n",
    "    for i, batch in enumerate(data_iterator):\n",
    "        data1 = batch.data[0].as_in_context(ctx)\n",
    "        data2 = batch.data[1].as_in_context(ctx)\n",
    "        data = [data1, data2]\n",
    "        label = batch.label[0].as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        metric.update([label], [output])\n",
    "    return metric.get()[1]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
